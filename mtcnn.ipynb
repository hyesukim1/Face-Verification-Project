{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mtcnn.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMactwwPmvDB63Akj1k3Uxo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyesukim1/Face-Verification-Project/blob/main/mtcnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# mctnn 모델\n"
      ],
      "metadata": {
        "id": "hajlTDmqEFS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MTCNN(nn.Module):\n",
        "    \"\"\"MTCNN face detection module.\n",
        "\n",
        "    This class loads pretrained P-, R-, and O-nets and, given raw input images as PIL images,\n",
        "    returns images cropped to include the face only. Cropped faces can optionally be saved also.\n",
        "    \n",
        "    Keyword Arguments:\n",
        "        image_size {int} -- Output image size in pixels. The image will be square. (default: {160})\n",
        "        margin {int} -- Margin to add to bounding box, in terms of pixels in the final image. \n",
        "            Note that the application of the margin differs slightly from the davidsandberg/facenet\n",
        "            repo, which applies the margin to the original image before resizing, making the margin\n",
        "            dependent on the original image size. (default: {0})\n",
        "        min_face_size {int} -- Minimum face size to search for. (default: {20})\n",
        "        thresholds {list} -- MTCNN face detection thresholds (default: {[0.6, 0.7, 0.7]})\n",
        "        factor {float} -- Factor used to create a scaling pyramid of face sizes. (default: {0.709})\n",
        "        prewhiten {bool} -- Whether or not to prewhiten images before returning. (default: {True})\n",
        "        select_largest {bool} -- If True, if multiple faces are detected, the largest is returned.\n",
        "            If False, the face with the highest detect probability is returned. (default: {True})\n",
        "        keep_all {bool} -- If True, all detected faces are returned, in the order dictated by the\n",
        "            select_largest parameter. If a save_path is specified, the first face is saved to that\n",
        "            path and the remaining faces are saved to <save_path>1, <save_path>2 etc.\n",
        "        device {torch.device} -- The device on which to run neural net passes. Image tensors and\n",
        "            models are copied to this device before running forward passes. (default: {None})\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, image_size=160, margin=0, min_face_size=20,\n",
        "        thresholds=[0.6, 0.7, 0.7], factor=0.709, prewhiten=True,\n",
        "        select_largest=True, keep_all=False, device=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.margin = margin\n",
        "        self.min_face_size = min_face_size\n",
        "        self.thresholds = thresholds\n",
        "        self.factor = factor\n",
        "        self.prewhiten = prewhiten\n",
        "        self.select_largest = select_largest\n",
        "        self.keep_all = keep_all\n",
        "\n",
        "        self.pnet = PNet()\n",
        "        self.rnet = RNet()\n",
        "        self.onet = ONet()\n",
        "\n",
        "        self.device = torch.device('cpu')\n",
        "        if device is not None:\n",
        "            self.device = device\n",
        "            self.to(device)\n",
        "\n",
        "\n",
        "    def forward(self, img, save_path=None, return_prob=False):\n",
        "        \"\"\"Run MTCNN face detection on a PIL image. This method performs both detection and\n",
        "        extraction of faces, returning tensors representing detected faces rather than the bounding\n",
        "        boxes. To access bounding boxes, see the MTCNN.detect() method below.\n",
        "        \n",
        "        Arguments:\n",
        "            img {PIL.Image} -- A PIL image.\n",
        "        \n",
        "        Keyword Arguments:\n",
        "            save_path {str} -- An optional save path for the cropped image. Note that when\n",
        "                self.prewhiten=True, although the returned tensor is prewhitened, the saved face\n",
        "                image is not, so it is a true representation of the face in the input image.\n",
        "                (default: {None})\n",
        "            return_prob {bool} -- Whether or not to return the detection probability.\n",
        "                (default: {False})\n",
        "        \n",
        "        Returns:\n",
        "            Union[torch.Tensor, tuple(torch.tensor, float)] -- If detected, cropped image of a face\n",
        "                with dimensions 3 x image_size x image_size. Optionally, the probability that a\n",
        "                face was detected. If self.keep_all is True, n detected faces are returned in an\n",
        "                n x 3 x image_size x image_size tensor with an optional list of detection\n",
        "                probabilities.\n",
        "\n",
        "        Example:\n",
        "        >>> from facenet_pytorch import MTCNN\n",
        "        >>> mtcnn = MTCNN()\n",
        "        >>> face_tensor, prob = mtcnn(img, save_path='face.png', return_prob=True)\n",
        "        \"\"\"\n",
        "\n",
        "        with torch.no_grad():\n",
        "            boxes, probs = self.detect(img)\n",
        "\n",
        "            if boxes is None:\n",
        "                if return_prob:\n",
        "                    return None, [None] if self.keep_all else None\n",
        "                else:\n",
        "                    return None\n",
        "\n",
        "            if not self.keep_all:\n",
        "                boxes = boxes[[0]]\n",
        "\n",
        "            faces = []\n",
        "            for i, box in enumerate(boxes):\n",
        "                face_path = save_path\n",
        "                if save_path is not None and i > 0:\n",
        "                    save_name, ext = os.path.splitext(save_path)\n",
        "                    face_path = save_name + '_' + str(i + 1) + ext\n",
        "\n",
        "                face = extract_face(img, box, self.image_size, self.margin, face_path)\n",
        "                if self.prewhiten:\n",
        "                    face = prewhiten(face)\n",
        "                faces.append(face)\n",
        "\n",
        "            if self.keep_all:\n",
        "                faces = torch.stack(faces)\n",
        "            else:\n",
        "                faces = faces[0]\n",
        "                probs = probs[0]\n",
        "\n",
        "            if return_prob:\n",
        "                return faces, probs\n",
        "            else:\n",
        "                return faces\n",
        "\n",
        "    def detect(self, img):\n",
        "        \"\"\"Detect all faces in PIL image and return bounding boxes.\n",
        "\n",
        "        This method is used by the forward method and is also useful for face detection tasks\n",
        "        that require lower-level handling of bounding boxes (e.g., face tracking). The\n",
        "        functionality of the forward function can be emulated by using this method followed by\n",
        "        the extract_face() function.\n",
        "        \n",
        "        Arguments:\n",
        "            img {PIL.Image} -- A PIL image.\n",
        "        \n",
        "        Returns:\n",
        "            tuple(numpy.ndarray, list) -- For N detected faces, a tuple containing an\n",
        "                Nx4 array of bounding boxes and a length N list of detection probabilities.\n",
        "                Returned boxes will be sorted in descending order by detection probability if\n",
        "                self.select_largest=False, otherwise the largest face will be returned first.\n",
        "\n",
        "        Example:\n",
        "        >>> from PIL import Image, ImageDraw\n",
        "        >>> from facenet_pytorch import MTCNN, extract_face\n",
        "        >>> mtcnn = MTCNN(keep_all=True)\n",
        "        >>> boxes, probs = mtcnn.detect(img)\n",
        "        >>> # Draw boxes and save faces\n",
        "        >>> img_draw = img.copy()\n",
        "        >>> draw = ImageDraw.Draw(img_draw)\n",
        "        >>> for i, box in enumerate(boxes):\n",
        "        ...     draw.rectangle(box.tolist())\n",
        "        ...     extract_face(img, box, save_path='detected_face_{}.png'.format(i))\n",
        "        >>> img_draw.save('annotated_faces.png')\n",
        "        \"\"\"\n",
        "\n",
        "        with torch.no_grad():\n",
        "            boxes = detect_face(\n",
        "                img, self.min_face_size,\n",
        "                self.pnet, self.rnet, self.onet,\n",
        "                self.thresholds, self.factor,\n",
        "                self.device\n",
        "            )\n",
        "\n",
        "        if len(boxes) == 0:\n",
        "            return None, [None]\n",
        "\n",
        "        if self.select_largest:\n",
        "            boxes = boxes[\n",
        "                np.argsort(\n",
        "                    (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
        "                )[::-1]\n",
        "            ]\n",
        "        \n",
        "        return boxes[:, 0:4], boxes[:, 4].tolist()"
      ],
      "metadata": {
        "id": "m2aXIox4EDTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting embeddings"
      ],
      "metadata": {
        "id": "IZyHimkV_89a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install facenet-pytorch"
      ],
      "metadata": {
        "id": "9HjxW4teAliK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "BnpAmQocAAXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Face recognition from images"
      ],
      "metadata": {
        "id": "EWjX3JPVAGsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model is running on CPU, since it is already pre-trained and doesnt require GPU\n",
        "device = torch.device('cpu') \n",
        "print('Running on device: {}'.format(device))"
      ],
      "metadata": {
        "id": "jy_NWzDqAsVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define MTCNN module\n",
        "#Since MTCNN is a collection of neural nets and other code, \n",
        "#The device must be passed in the following way to enable copying of objects when needed internally.\n",
        "mtcnn = MTCNN(\n",
        "    image_size=160, margin=0, min_face_size=20,\n",
        "    thresholds=[0.6, 0.7, 0.7], factor=0.709, prewhiten=True,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "i7Q1CfSxAvOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function takes 2 vectors 'a' and 'b'\n",
        "#Returns the cosine similarity according to the definition of the dot product\n",
        "def cos_sim(a, b):\n",
        "    dot_product = np.dot(a, b)\n",
        "    norm_a = np.linalg.norm(a)\n",
        "    norm_b = np.linalg.norm(b)\n",
        "    return dot_product / (norm_a * norm_b)\n",
        "\n",
        "#cos_sim returns real numbers,where negative numbers have different interpretations.\n",
        "#So we use this function to return only positive values.\n",
        "def cos(a,b):\n",
        "    minx = -1 \n",
        "    maxx = 1\n",
        "    return (cos_sim(a,b)- minx)/(maxx-minx)\n",
        "\n",
        "# Define Inception Resnet V1 module (GoogLe Net)\n",
        "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "\n",
        "# Define a dataset and data loader\n",
        "dataset = datasets.ImageFolder('Film/Test')\n",
        "dataset.idx_to_class = {i:c for c, i in dataset.class_to_idx.items()}\n",
        "loader = DataLoader(dataset, collate_fn=lambda x: x[0])\n",
        "\n",
        "#Perfom MTCNN facial detection\n",
        "#Detects the face present in the image and prints the probablity of face detected in the image.\n",
        "aligned = []\n",
        "names = []\n",
        "for x, y in loader:\n",
        "    x_aligned, prob = mtcnn(x, return_prob=True)\n",
        "    if x_aligned is not None:\n",
        "        print('Face detected with probability: {:8f}'.format(prob))\n",
        "        aligned.append(x_aligned)\n",
        "        names.append(dataset.idx_to_class[y])\n",
        "\n",
        "# Calculate the 512 face embeddings\n",
        "aligned = torch.stack(aligned).to(device)\n",
        "embeddings = resnet(aligned).cpu()\n",
        "\n",
        "# Print distance matrix for classes.\n",
        "#The embeddings are plotted in space and cosine distace is measured.\n",
        "cos_sim = nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
        "for i in range(0,len(names)):\n",
        "    emb=embeddings[i].unsqueeze(0)\n",
        "    # The cosine similarity between the embeddings is given by 'dist'.\n",
        "    dist =cos(embeddings[0],emb)  \n",
        "        \n",
        "dists = [[cos(e1,e2).item() for e2 in embeddings] for e1 in embeddings]\n",
        "# The print statement below is\n",
        "#Helpful for analysing the results and for determining the value of threshold.\n",
        "print(pd.DataFrame(dists, columns=names, index=names)) "
      ],
      "metadata": {
        "id": "rc3arGQxAzQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from facenet_pytorch import MTCNN, InceptionResnetV1,extract_face\n",
        "from PIL import Image,ImageDraw\n",
        "import torch\n",
        "import cv2\n",
        "import torch.nn as nn\n",
        "\n",
        "#Takes 2 vectors 'a' and 'b'.\n",
        "#Return the cosine similarity according to the definition of the dot product.\n",
        "def cos_sim(a, b):\n",
        "    dot_product = np.dot(a, b)\n",
        "    norm_a = np.linalg.norm(a)\n",
        "    norm_b = np.linalg.norm(b)\n",
        "    return dot_product / (norm_a * norm_b)\n",
        "\n",
        "#cos_sim returns real numbers,where negative numbers have different interpretations.\n",
        "#So we use this function to return only positive values.\n",
        "def cos(a,b):   \n",
        "    minx = -1 \n",
        "    maxx = 1\n",
        "    return (cos_sim(a,b)- minx)/(maxx-minx)\n",
        "\n",
        "\n",
        "#This is the function for doing face recognition.\n",
        "def verify(embedding): \n",
        "    for i,k in enumerate(embeddings):\n",
        "        for j,l in enumerate(embedding):\n",
        "            #Computing Cosine distance.\n",
        "            dist =cos(k,l)\n",
        "               \n",
        "            # Chosen threshold is 0.85. \n",
        "            #Threshold is determined after seeing the table in the previous cell.\n",
        "            if dist > 0.85:\n",
        "                #Name of the person identified is printed on the screen, as well as below the detecetd face (below the rectangular box).\n",
        "                text=names[i]\n",
        "                cv2.putText(im, text,(boxes[j][0].astype(int) ,boxes[j][3].astype(int) + 17), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255,255,255), 2)\n",
        "                print(text)\n",
        "                \n",
        "#Model running on CPU           \n",
        "device = torch.device('cpu')  \n",
        "\n",
        "#Define Inception Resnet V1 module (GoogLe Net)\n",
        "resnet = InceptionResnetV1(pretrained='vggface2').eval().to('cpu')\n",
        "\n",
        "#Define MTCNN module\n",
        "#Since MTCNN is a collection of neural nets and other code, \n",
        "#The device must be passed in the following way to enable copying of objects when needed internally.\n",
        "#'keep_all' is kept True. All the faces present in the image will be detected.\n",
        "mtcnn = MTCNN(\n",
        "    image_size=160, margin=0, min_face_size=20,\n",
        "    thresholds=[0.6, 0.7, 0.7], factor=0.709, prewhiten=True,\n",
        "    device=device,keep_all=True\n",
        ")\n",
        "\n",
        "#Get cropped and prewhitened image tensor of PIL image.\n",
        "img = Image.open('Film/Test/1.jpg')\n",
        "img_cropped = mtcnn(img)\n",
        "boxes,prob=mtcnn.detect(img)\n",
        "img_draw = img.copy()\n",
        "draw = ImageDraw.Draw(img_draw)\n",
        "\n",
        "#Rectangular boxes are drawn on faces present in the image.\n",
        "#The detected and cropped faces are then saved.\n",
        "for i, box in enumerate(boxes):\n",
        "    draw.rectangle(box.tolist())\n",
        "    extract_face(img, box, save_path='Film/Test/Cropped_Face_{}.jpg'.format(i))\n",
        "img_draw.save('Film/Test/Faces_Detected.jpg')\n",
        "\n",
        "#Calculate embeddings of each cropped face and print it.\n",
        "im=cv2.imread('Film/Test/Faces_Detected.jpg')\n",
        "img_embedding = resnet(img_cropped)\n",
        "print(img_embedding)\n",
        "\n",
        "#print(size of img_embedding)\n",
        "print(img_embedding.size())\n",
        "\n",
        "#Call function verify. \n",
        "#Identify the person with the help of embeddings.\n",
        "cos_sim = nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
        "verify(img_embedding)\n",
        "\n",
        "#'Image' window opens.\n",
        "#The PIL image now have rectangular boxes on detected faces.\n",
        "#The identified faces have their respective name below the box.\n",
        "cv2.imshow(\"Image\",im)\n",
        "k=cv2.waitKey(0)\n",
        "\n",
        "#13 is for 'Enter' key.\n",
        "#If 'Enter' key is pressed, all the windows are made to close forcefully.\n",
        "if k==13:\n",
        "    cv2.destroyAllWindows()    "
      ],
      "metadata": {
        "id": "p4535SsdAEdG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}